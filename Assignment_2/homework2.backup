# -*- coding: utf-8 -*-
# Todo:
    # - Pre-trained model
    # - Model ensemble
    # - Extract features using CNN, then XGBoost

import numpy as np
import pickle
np.random.seed(7)  # for reproducibility

from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold

from keras.models import Sequential, Model
from keras.layers import Dense, Dropout, Flatten, Activation
from keras.layers import Convolution2D, MaxPooling2D, UpSampling2D
from keras.regularizers import l2

import keras
from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator
from keras.utils import np_utils
from keras import backend as K
K.set_image_dim_ordering('tf')

## TRAIN, VALIDATION SET  
# X has dimension 32x32x3 and is already normalized
with open('train_feat.pickle','rb') as f:
    X_train_label = pickle.load(f)
X_train = X_train_label.astype('float32')


with open('validation_feat.pickle','rb') as f:
    X_valid_label = pickle.load(f)
X_valid = X_valid_label.astype('float32')

#Y has 10 classes and need one-hot encoding
with open('train_lab.pickle','rb') as f:
    Y_train_label = pickle.load(f)
Y_train = np_utils.to_categorical(Y_train_label, 10)

with open('validation_lab.pickle','rb') as f:
    Y_valid_label = pickle.load(f)
Y_valid = np_utils.to_categorical(Y_valid_label, 10)

# Parameter
num_classes = Y_train.shape[1]
batch_size = 64

# data augmentation
data_generator = ImageDataGenerator( \
    rotation_range=30, \
    width_shift_range=0.2, \
    height_shift_range=0.2, \
    fill_mode='nearest',
    horizontal_flip=True)
train_gen = data_generator.flow(X_train, Y_train, batch_size=batch_size)

# create model
def create_cnn_model():
    model = Sequential()
    model.add(Convolution2D(32, 3, 3, input_shape=(32, 32, 3), activation='relu', border_mode='same'))
    model.add(Dropout(0.2))
    model.add(Convolution2D(32, 3, 3, activation='relu', border_mode='same'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Convolution2D(64, 3, 3, activation='relu', border_mode='same'))
    model.add(Dropout(0.2))
    model.add(Convolution2D(64, 3, 3, activation='relu', border_mode='same'))
    model.add(MaxPooling2D(pool_size=(2, 2)))
    #model.add(Convolution2D(128, 3, 3, activation='relu', border_mode='same'))
    #model.add(Dropout(0.2))
    #model.add(Convolution2D(128, 3, 3, activation='relu', border_mode='same'))
    #model.add(MaxPooling2D(pool_size=(2, 2)))
    model.add(Flatten())
    model.add(Dropout(0.2))
    model.add(Dense(1024, activation='relu', W_regularizer=l2(0.01)))
    model.add(Dropout(0.2))
    model.add(Dense(512, activation='relu', W_regularizer=l2(0.01)))
    model.add(Dropout(0.2))
    model.add(Dense(10, activation='softmax'))
    # Compile model
    model.compile(loss='categorical_crossentropy', optimizer='Adam', metrics=['accuracy'])
    return model

early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.00001, patience=30, verbose=2, mode='auto')

## BASIC DEEP NETWORK MODEL - 32.6%
epochs = 100
model = create_cnn_model()
model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), nb_epoch=epochs, \
                batch_size=batch_size, verbose=2)
# Final evaluation of the model
scores = model.evaluate(X_valid, Y_valid, verbose=0)
print("CNN Error: %.2f%%" % (100-scores[1]*100))


epochs = 100
model = create_cnn_model()
model.fit_generator(train_gen, samples_per_epoch=len(X_train), validation_data=(X_valid, Y_valid), nb_epoch=epochs, \
                verbose=2)
# Final evaluation of the model
scores = model.evaluate(X_valid, Y_valid, verbose=0)
print("CNN Error: %.2f%%" % (100-scores[1]*100))


# BASIC DEEP NETWORK WITH CROSS VALIDATION
kfold = KFold(n_splits=5, shuffle=True, random_state=13)
scores_list = []
model_list = []
for train, test in kfold.split(X_train, Y_train):
    model = None
    model = create_cnn_model()
    model.fit(X_train[train], Y_train[train], validation_data=(X_train[test], Y_train[test]), \
                 nb_epoch=200, batch_size=200, verbose=2, callbacks=[early_stop])
    # evaluate the model
    scores = model.evaluate(X_valid, Y_valid, verbose=0)
    scores_list.append(scores[1])
    model_list.append(model)

print("CNN Error: %.2f%%" % (100-np.mean(scores_list)*100))


scores_list = []
model_list = []
for i in np.arange(10):
    model = None
    model = create_cnn_model()
    model.fit(X_train, Y_train, validation_data=(X_valid, Y_valid), nb_epoch=epochs, \
                batch_size=64, verbose=2)
    # evaluate the model
    scores = model.evaluate(X_valid, Y_valid, verbose=0)
    scores_list.append(scores[1])
    model_list.append(model)

print("CNN Error: %.2f%%" % (100-np.mean(scores_list)*100))

# Ensemble deep network model - 27.5% with 10 models
predictions = np.zeros((Y_valid.shape[0],Y_valid.shape[1]))
for model in model_list:
    prediction = model.predict_classes(X_valid, batch_size=200, verbose=0)
    prediction_onehot = np_utils.to_categorical(prediction, 10)
    predictions = predictions + prediction_onehot

predictions_label = np.argmax(predictions, axis=1)
evaluate = np.equal(predictions_label, Y_valid_label)
print("Ensemble Error: %.2f%%" % (100-np.mean(evaluate)*100))

## MODEL WITH REPRESENTATION LEARNING FROM DEEP NETWORK
# Representation Model 1
representation_model = Model(input=model.input,
                                 output=model.layers[2].output)
X_train_representation = representation_model.predict(X_train)
X_valid_representation = representation_model.predict(X_valid)

# Representation Model 2
autoencoder_model = Sequential()
autoencoder_model.add(Convolution2D(32, 3, 3, input_shape=(32, 32, 3), activation='relu', border_mode='same'))
autoencoder_model.add(Convolution2D(32, 3, 3, activation='relu', border_mode='same'))
autoencoder_model.add(MaxPooling2D(pool_size=(2, 2)))
autoencoder_model.add(Convolution2D(64, 3, 3, activation='relu', border_mode='same'))
autoencoder_model.add(Convolution2D(64, 3, 3, activation='relu', border_mode='same'))
autoencoder_model.add(MaxPooling2D(pool_size=(2, 2)))
# at this point the representation is (8, 4, 4) i.e. 128-dimensional
autoencoder_model.add(Convolution2D(64, 3, 3, activation='relu', border_mode='same'))
autoencoder_model.add(Convolution2D(64, 3, 3, activation='relu', border_mode='same'))
autoencoder_model.add(UpSampling2D((2, 2)))
autoencoder_model.add(Convolution2D(32, 3, 3, activation='relu', border_mode='same'))
autoencoder_model.add(Convolution2D(32, 3, 3, activation='relu', border_mode='same'))
autoencoder_model.add(UpSampling2D((2, 2)))
autoencoder_model.add(Convolution2D(3, 3, 3, activation='sigmoid', border_mode='same'))

autoencoder_model.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])
autoencoder_model.fit(X_train, X_train,
                nb_epoch=100,
                batch_size=64,
                shuffle=True,
                validation_data=(X_valid, X_valid))
# callbacks=[TensorBoard(log_dir='/tmp/autoencoder')]
from keras.layers import Input
input_img = Input(shape=(32, 32, 3))

x = Convolution2D(16, 3, 3, activation='relu', border_mode='same')(input_img)
x = MaxPooling2D((2, 2), border_mode='same')(x)
x = Convolution2D(8, 3, 3, activation='relu', border_mode='same')(x)
x = MaxPooling2D((2, 2), border_mode='same')(x)
x = Convolution2D(8, 3, 3, activation='relu', border_mode='same')(x)
encoded = MaxPooling2D((2, 2), border_mode='same')(x)

# at this point the representation is (8, 4, 4) i.e. 128-dimensional

x = Convolution2D(8, 3, 3, activation='relu', border_mode='same')(encoded)
x = UpSampling2D((2, 2))(x)
x = Convolution2D(8, 3, 3, activation='relu', border_mode='same')(x)
x = UpSampling2D((2, 2))(x)
x = Convolution2D(16, 3, 3, activation='relu', border_mode='same')(x)
x = UpSampling2D((2, 2))(x)
decoded = Convolution2D(3, 3, 3, activation='sigmoid', border_mode='same')(x)

autoencoder = Model(input_img, decoded)
autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])             
autoencoder.fit(X_train, X_train,
                nb_epoch=50,
                batch_size=256,
                shuffle=True,
                validation_data=(X_valid, X_valid), verbose=2)             


#------
from keras.datasets import mnist
import numpy as np

(X_train, _), (X_test, _) = mnist.load_data()
X_train = X_train.astype('float32') / 255.
X_test = X_test.astype('float32') / 255.

X_train2 = X_train.reshape(len(X_train),3072)
X_valid2 = X_valid.reshape(len(X_valid),3072)
input_img = Input(shape=(3072,))
encoded = Dense(128, activation='relu')(input_img)
encoded = Dense(64, activation='relu')(encoded)
encoded = Dense(32, activation='relu')(encoded)

decoded = Dense(64, activation='relu')(encoded)
decoded = Dense(128, activation='relu')(decoded)
decoded = Dense(3072, activation='sigmoid')(decoded)

autoencoder = Model(input=input_img, output=decoded)
autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy', metrics=['accuracy'])
autoencoder.fit(X_train2, X_train2,
                nb_epoch=50,
                batch_size=256,
                shuffle=True,
                validation_data=(X_valid2, X_valid2), verbose=2) 

representation_model = Model(input=autoencoder_model.input,
                                 output=autoencoder_model.layers[5].output)
representation_model.add(Flatten())
X_train_representation = representation_model.predict(X_train)
X_valid_representation = representation_model.predict(X_valid)

# SVM
from sklearn import svm
svm_model = svm.SVC(decision_function_shape='ovo')
svm_model.fit(X_train_representation, Y_train_label)
scores = svm_model.score(X_valid_representation, Y_valid_label)
print("SVM Error: %.2f%%" % (100-np.mean(scores)*100)) #63.2%

# Random Forest
from sklearn.ensemble import RandomForestClassifier
rf_model = RandomForestClassifier(n_estimators=100, max_features='auto')
rf_model.fit(X_train_representation, Y_train_label)
scores = rf_model.score(X_valid_representation, Y_valid_label)
print("Random Forest Error: %.2f%%" % (100-np.mean(scores)*100)) #64%

# Boosted Tree
from sklearn.ensemble import AdaBoostClassifier
from sklearn.tree import DecisionTreeClassifier
bt_model = AdaBoostClassifier(
    DecisionTreeClassifier(max_depth=2),
    n_estimators=600,
    learning_rate=1)
bt_model.fit(X_train_representation, Y_train_label)
scores = bt_model.score(X_valid_representation, Y_valid_label)
print("AdaBoost Error: %.2f%%" % (100-np.mean(scores)*100)) #

# XGBoost
import xgboost as xgb
xg_train = xgb.DMatrix( X_train_representation, label=Y_train_label)
xg_test = xgb.DMatrix(X_valid_representation, label=Y_valid_label)
# setup parameters for xgboost
param = {}
param['booster'] = 'gbtree'
# use softmax multi-class classification
param['objective'] = 'multi:softmax'
# scale weight of positive examples
param['eta'] = 0.1
param['gamma'] = 0.5290 #
#params['min_child_weight'] = 10
#params['colsample_bytree'] = 0.7
param['subsample'] = 0.7
param['max_depth'] = 7
param['silent'] = 1
param['num_class'] = 10
watchlist = [ (xg_train,'train'), (xg_test, 'test') ]

num_round = 1000
early_stopping = 25
bst = xgb.train(param, xg_train, num_round, watchlist, early_stopping_rounds=early_stopping );
# get prediction
pred = bst.predict( xg_test );

print ('predicting, classification error=%f' % (sum( int(pred[i]) != Y_valid_label[i] for i in range(len(Y_valid_label))) \
                                    / float(len(Y_valid_label)) ))

